---
layout: post
title: "AI and Its Perceived Dangers, What-Ifs and Creation of Your Own Reality Eno 7 9 24 21"
pubDate: 2021-09-24
author: Dhammarato
categories: [transcripts, Dhamma Talk]
tags: [transcripts, sati, mindfulness, meditation, dukkha, sukkha, sankhara, vedana, noble-eightfold-path, one-on-one, nurturing, compassion, brahma-viharas, philosophy, ethics, artificial-intelligence, fear, judgment, habit, friendship, reality, perception]
image: "https://i.ytimg.com/vi/d_GiHD6UqVM/maxresdefault.jpg"
description: "Transcript of September 24, 2021 Dhamma Talk with Dhammarato and Friends"
featured: false
hidden: false
toc: true
assemblyai_transcript_id: fa4326fd-d322-46f4-a2ce-9d3239fb0824
---

## AI and Its Perceived Dangers, What-Ifs and Creation of Your Own Reality Eno 7 9 24 21

### Video


<p><iframe style="width:100%;" height="315" src="https://www.youtube.com/embed/d_GiHD6UqVM?rel=0&amp;showinfo=0" frameborder="0" allowfullscreen></iframe></p>


### Transcript


**Speaker A:** All right,

**Speaker B:** okay. So as I was saying, I have— I feel like I have gotten a big kind of core idea of the practice overall, and that's kind of the

**Speaker A:** goal

**Speaker B:** is I've just caught myself at many moments during the week where being in a state of want and just noticing that, hey, I can, instead of having that energy diverted to there, I can divert it to being relaxed and being in the moment. And, you know, it's as simple as that, you know. And just noticing that, like, why, you know, why would I want to be there, you know, like, in that? It's just so pointless.

**Speaker A:** And just— well, there's a simple answer to that, and that is you were trained that way. Yeah, and you had all of the equipment that you needed to learn that lesson, and you became good at it. But look what it got you. Not— I mean, it got you a whole lot. Be thankful for what it did get you, but it also kind of left you with a broken heart.

**Speaker B:** That's a really good point. Yeah, totally. Yeah, absolutely. So I don't know, it's just been— I've just been taking that lesson of why spend energy wanting things when you can spend energy being grateful for what you have in this current moment, you know?

**Speaker A:** That's like the cherry on top of life, or that's actually getting above it all in the sense that before you had a mixture, there was some good but they came with the bad. Now you recognize that the bad was, how to say it, in a way, it's the original sin of humanity. That it is Adam and Eve, it is our judgment. But the important part is to recognize that it's not the good and the bad, which is judgment, but it's actually the good and the bad here is already making that judgment, that what we're really needing to do is to stop the judgments and to be satisfied, and you're beginning to see that. Yeah, being satisfied right now, I mean, I don't have to worry about what's good and what's bad anymore. Not my problem now.

**Speaker B:** Yeah, totally. So yeah, I've— and through there, through like just appreciating that, I've— I don't know, I've had just some really wonderful moments through the week. Like I was just listening to some music and it was just like— it was just playing and it was just like God, this is so incredible. It was just like such a, like, a lasting— like, usually, like, if I enjoy a part of the music, it'd be like, wow, that, that part was really great. And they're like, oh, that part was really great. And it's like, you're listening, it's like,

**Speaker A:** oh,

**Speaker B:** but that part was really good, you know. It wasn't like that. I was just like, it was just like, wow, you know, this is wonderful. And there was, there was, I don't know, it just felt like the sustaining was there, you know, which was really nice. Yeah.

**Speaker A:** Could it be possible to say it like this, that in fact the enjoyable music is the music that we're actually listening to? And what was happening with you in the old days was that there was a part of the song that you were listening to, and so you became attached to that. And so the rest of the song you weren't listening to, you were continuing to listen to what was there while the other was just noise coming in.

**Speaker B:** Yeah, totally. Absolutely.

**Speaker A:** But now you're able to stay with this particular music as it's going note by note. And not only that, but you're beginning to pick up a whole lot of stuff. An example would be to start with a trio or a small band and begin to pay attention to actually hearing all three instruments working together as well as the outcome of it, so that you're actually really getting into it. And then you can begin to do that with symphony orchestras if you can.

**Speaker B:** Yeah, totally.

**Speaker A:** Yeah, right. So that you really begin to listen to what actually is happening with each instrument and how it fits into the thing and really crawl into the music. That's just— it's enjoyable. That's what music should be. But much of the music winds up just trying to get our attention or grasp the attention. And so we wind up being frantic. In a way, that we— in fact, Henry David Thoreau had the expression, "We live lives of quiet desperation," and so while we're listening to the music, in a way, our own desperation interferes with our ability to really pay attention to what's going on.

**Speaker B:** Yeah, absolutely, yeah, absolutely. I mean, yeah, and just another moment I was just like looking at the sea and it was just like, wow, just so—

**Speaker A:** And so there is another way of doing that, but it's exactly the same thing in the sense that one of the parts of being a trained psychologist, the training is, if there's actually training at it, is the training in paying attention, to really look, listen, and to keep track of what's going on, and to integrate all of the breathing and the facial expressions and everything that's happening. That in fact, when someone is doing that, they've got information that many of the people in the in the room or in the crowd don't have. It's right there in front of them, but they're so busy thinking about what's going on instead of actually paying close attention to what's going on. And so these are skills that the psychologists pick up that could be thought of and called sometimes mind reading, but it's not actually mind reading, it's just reading the information that's available that most people are not paying attention to, that the Buddha was actually quite specific on this, and Ajahn Po made a point of rubbing it in more than once with me, is that you cannot tell what is actually in a person's mind, that all you can do is look for congruences and incongruences in their behavior, but that the important thing is that you've got to pay attention, you've got to watch what's going on. And that fit in so well with the lesson of psychology of the actual training process. And to be honest with you, when I was in training, I wasn't very good at it. But when I became a monk and started working with people, those old skills started to manifest themselves. Because the real issue was the paying attention that is Sati, to keep remembering, to keep looking, keep watching what's going on. And that was basically what you were doing, and you got right into the music. That's part of the reason why these conversations with my friends is so enjoyable. Is because it gives me something to do. Hey, Bhikkhuni.

**Speaker B:** Yeah, totally. That's very interesting. Yeah, totally. And furthermore, another thing that I've just really been appreciating is just that feeling of friendship with Because again, like, it's so easy. Like, I don't want to say especially in a philosophy class, but it can happen that in a philosophy class, like, you say something and then another person says something and then you're kind of competing, you know, to be like the smart one or whatever. You know, I can tell when that, you know, it arises from time to time. But, and then it's just, it's, and then it can be very frustrating because it's like, I don't want to be feeling this at all right now. Like, really, it's a horrible feeling. So then just remembering that, like, I'm just friends with this person, you know, like, it's nothing more than that, you know. And that's a very enjoyable way. And just like passing people on the street and just being like, you know, we're friends, you know. Were in a very deep way, you know, like, yeah, this, you know, and yeah, with things other than people, you know, with like, I don't know, I think about like, I'm friends with the ground, you know. I don't know, it works for me because it just feels right. Or like, yeah, so I've been really enjoying that.

**Speaker A:** Yes, going back to the point about philosophy that we were discussing before, this is actually part of it in the sense that you can think of the really, really old ancient philosophers back in the Greek days, or in fact they keep going back in time. The problem is recorded history as the opposed to. And so all of a sudden there's a dark past back there where we don't really know what was going on, but there had to have been Buddhas and Socrates and Jesuses and— well, I could actually put Plato in there, I guess, and all. But the point is that those guys back then had a love for philosophy in the sense of the loving of looking at what was going on and getting a big kick out of it, while we were appreciating the reality of the situation. And that what modern philosophy has become is not studying the reality of nature, what we've been doing instead is studying the nature of old philosophers. Instead of studying what they were actually studying. That I can actually see Nietzsche when he says that God is dead and I am free because it's his God that is dead and he is free. And it makes absolute perfect sense when you are Nietzsche, but it makes absolutely no sense at all to your average Christian college student taking 101 philosophy. Philosophy, that doesn't make any sense to them. How could God be dead? And so, the point being is that now, you are actually becoming a philosopher, rather than studying old philosophers. Is falling in love with the reality of what's happening right now. Hence the word philo and soka. Totally,

**Speaker B:** totally. Yeah, um, yeah, I don't know. I really, you know, obviously I really enjoy philosophy.

**Speaker A:** Well, what you're saying to me in that statement is that you're enjoying the fact that there is an old history of people that really got into it and that they loved it. Yeah, right. Falling in love with the universe, but being friends with the ground itself, that's what we're getting at, isn't it? And that in somehow or another in modern philosophy, it's been the weight has changed from the love, the pillow, into the sofas. And so we think that if the more knowledge and the more information and the more wisdom that we have, the better off we'll be, missing actually the point. The point is the pillow and the sophie.

**Speaker B:** Yeah, absolutely.

**Speaker A:** Because a lot of people don't like what's going on.

**Speaker B:** Yeah, totally.

**Speaker A:** And so they want to change it,

**Speaker B:** or it comes down to that same, like, competitive thing where it's like, you know, you're trying to be better than old philosophers and whatnot, you know. And yeah, right.

**Speaker A:** To where they didn't care. I mean, the Stoics didn't care, and still don't, whether they're going to be studied the way that they were or have been studied. That they were out actually looking for the correct way of living, and in many cases they were spot on with the Buddha. I think that in fact there was something that was a little more sophisticated with the Buddha, in the sense that he saw morality as actually part of the outcome of an awakened mind, rather than using morality as a straitjacket to get the mind straight. And you see that often in Buddhism, but that's basically— Marcus Aurelius is a good example of that also, of you've got to be the most upstanding, correct, tell the truth, circumspect. But in fact, what's really going on is, is that someone who has gotten really in his own mind cleaned out and gotten above it all, that's kind of how he acts naturally.

**Speaker B:** Yeah, totally. Totally. I'm

**Speaker A:** very curious.

**Speaker B:** This is completely unrelated though, so I feel— philosophical topic or whatever, but I cannot help but bring it up just because I do.

**Speaker A:** No problem with me. You don't have to apologize and feel sorry. I know you're full of questions. I'm full of it sometimes too.

**Speaker B:** Totally. I'm curious, because I totally agree that that way of looking at

**Speaker A:** it is

**Speaker B:** probably one of the best ways of doing it. I'm not— yeah, but I'm just curious, what do you think for going into the future with— now that we're going

**Speaker A:** to never get there.

**Speaker B:** Well, I mean, you can even argue now, even right now with artificial intelligence, really is my question. Like, how do you— there's a big question of like, you know, how are we going to program artificial intelligence to be ethical, do you see Buddhist ethics as being a potential best solution for that?

**Speaker A:** Well, if it's the wise outcome and artificial intelligence is more wise than, let us say, polluted natural intelligence, that in fact one of the things that we can say is that a computer can be turned off and turned back on, and the computer itself doesn't care. But if I come over there and turn you off and then turn you back on, you might have something to say about it. Yeah, okay. In that regard, that's the point that we have to keep the issue that this is artificial. Every machine can be turned off and turned back on again. That's an important quality. And so also what that means is he who controls the power switch of the machine is in charge. Okay, this is an important quality. Whether or not the machine is doing its job correctly or not is just little smaller switches that someone down the line has some control over. Okay. And so basically, AI machines are nothing but pattern recognition hardware that's built much more like a graphics processor that is parallelly capable of making very fast binary divisions and multiplications so as to reframe graphics. And so it's actually most of AI is nothing but fiddling with graphics, drawing dots, making connections, seeing how far the eyes are, how far the lens is from the camera, making predictions and all of this kind of stuff. For facial recognitions, it's not a big deal. In fact, I was kind of interested in AI back in the 1980s. It's made quite a revolution, but the real revolution has been made in the hardware, not in the software. We already back then knew what we needed to do, we just didn't have the hardware to do it. Now, especially Elon Musk who's got the hardware. Okay, so the whole point though is that machines do not have instincts, they do not have feelings. And in that regard, the machine may know that it's a machine, but it still has the so-what quality to it that you could almost say that AI When it's operating correctly, never mind who designed it, when it's working correctly, it's dealing with reality, and dealing with reality as if it were a toy or an abstraction, not important. You see, to you, getting turned off and back on again by a machine, that you don't like. Machines don't care whether they're turned off and on.

**Speaker B:** Totally.

**Speaker A:** Which means that AI will in fact become a marvelous teacher for human beings someday, not just with implants, but to figure out that we don't have to go around feeling bad about the decisions that we make. Instead, uh, AI, one of the best qualities about it is, is that it does not feel bad about making decisions and then making corrections. That AI is a fast learning machine, and humans are very slow at learning because we don't like to make mistakes. And AI is quite fine with making mistakes and learning from them.

**Speaker B:** Yeah, totally. Absolutely.

**Speaker A:** That's why Elon Musk wants as much road data as he could possibly put into those giant AI machines that he's got. I guess he's going to move them to Texas out of California. But in any case, getting as much data as you can makes you a much better driver if you're a human. Imagine now that you had all the data of all of the drivers in the United States packed into a model, and that's what is actually the self-driving system that they've got, that's AI. And so, that's the answer to your question, is that AI is going to help humans become better humans.

**Speaker B:** [Speaker:DAMMARATO] That's very interesting. I'm wondering specifically about say we have a— going farther into the future, we're probably going to design androids, right? Like robots that act like humans or

**Speaker A:** can mimic— and can't be turned off.

**Speaker B:** And yes, and will likely— well, I mean, I think it's Maslow's rules of our Is that Maslow's?

**Speaker A:** No, it's Ashtanoff.

**Speaker B:** Asmoff's, yeah, Asmoff's. Yes, it's getting mixed up.

**Speaker A:** How did I know that? Because I know what you're going to talk about, right? The rules of AI are— okay, and so

**Speaker B:** I think that self-preservation built into it.

**Speaker A:** Preservation of the human is more important than the self-preservation of the machine. And machines are capable of that easily. I mean, that's an easy thing to program into them if you've got that kind of AI going. Machine, bang bang. Hi friend. I mean, that's easy enough to program in. So what you're saying is, is the danger then is, is that some people will try to program in their own personal wickedness?

**Speaker B:** Absolutely.

**Speaker A:** Yeah. Is that possible? The answer is yes, that's possible. But that's the difference, actually, at one scale between a stick and a gun. The gun is just a whole lot easier to use, which means that people can act upon their weakest impulse forces with deadly force, to where if you had the number of murders that happen— okay, we're actually talking about murders— if every murder had to be done hand-to-hand with my hands all over areas of your body so that I could put your life out, that was the only way that murder could happen. It wouldn't have to be much of it because that's a lot of work. A bunch makes it really easy. Okay, so we're saying that yes, AI can be programmed with people with having ill intent just to make their ill intentions easier to perform. That's possible. It's probably because it's possible will likely happen.

**Speaker B:** Yeah, totally. I mean, it's already happened, but I

**Speaker A:** don't think it tests the whole history of AI. I don't think it tests the future of AI. I think that's an issue that has to be dealt with along the path, just like Facebook is an issue that needs to be dealt with along this path that we're on right now. Totally, totally. That's the disinformation that's out there. Totally.

**Speaker B:** I mean, it's— yeah.

**Speaker A:** Okay, so if AI is going towards reality then humanity will be of great benefit to that. But if it's going in the direction of greed, ill will, and let us say bifurcation or delusion, then those elements will need to be dealt with.

**Speaker B:** Totally. I suppose— I don't know, I'm just— what I might ask now might be too vague to be really answerable, but I'm curious because at the heart of Buddhist ethics, as far as I understand it, is that if you don't want anything, you're not going to do any wrong.

**Speaker A:** Well, that's kind of a side benefit, but go ahead. For sure.

**Speaker B:** So we want— One of the things

**Speaker A:** that makes it really joyful, by the way, is how Not mine.

**Speaker B:** But so we have, or we're going to have artificial intelligence. We're going to have the— we're going to want to have them to want to do good things, right? So we're going to have them wanting things then. And theoretically, that could go very badly, very quickly, right? With, you know, take the example, like, what if the artificial intelligence decide, like, okay, safety is what's important for the humans, we need to take care of them or whatever. So they trap us all in a box or something like that. I'm sure you've heard examples like that and kept us out or whatever, you know, because they're like, we're maximizing safety, we're maximizing human happiness. Keep them in there?

**Speaker A:** Why would they try? Well, maximizing safety and maximizing human happiness are not the same thing, so the AI has a quandary that it has to work out. Is it going to maximize safety, or is it going to maximize happiness?

**Speaker B:** But either way, we can see issues arising.

**Speaker A:** Well, why is it maximizing things? Why isn't dealing with reality as it is in a particular case, you see? Actually, what I think the problem is, is that we've given control of the buttons of the AI to the philosophers in universities reading books, rather than leaving it to the engineers to do practical things with this AI, like let it drive an automobile.

**Speaker B:** That's fair enough, but those practical questions are always going to have ethical Conundrums, you know, it's— we can break the

**Speaker A:** ethical conundrums are not in the AI, the ethical conundrums are in the humans. Let us hope that AI can point that out to the humans, that the problem is within the greed and the ill will and the delusions of individuals, whether they've got their fingers on the buttons of the AI machines themselves or whether they're the ones who have the buttons in their own minds about the AI machines, which is where the philosophers are.

**Speaker B:** Totally. So then to bring it to a specific philosophical question that is theoretically going

**Speaker A:** to— here's a question: are humans, never mind what AI is capable of doing, are humans going to give up the control to the AI machines willingly or not?

**Speaker B:** More than likely, I think.

**Speaker A:** Well, not all the humans all at the same time.

**Speaker B:** Totally. Yeah, totally.

**Speaker A:** Absolutely. That in fact, you, you were, uh, when you were talking before, one of the questions, one of the answers that came to mind, which was breathing, and I'll bring it now, is the answer is not in my neck of the woods. And there's a whole lot of people who have that mentality that will and shall remain

**Speaker B:** free.

**Speaker A:** Perhaps like people are afraid of the vaccines, not recognizing that it's more dangerous to do without the vaccine. Well, it may in fact be more dangerous to live without AI, but hey, I can get along without it. I mean, I live a dangerous life, but at least I'm free and happy and I don't need the AI. Totally, totally. But how is AI going to individually affect individuals anyway? Are we going to have I mean, um, a timer on a coffee pot, right? Maybe the coffee pot is smart enough to know when someone wakes up, but someone's still going to have to put coffee and water in the coffee pot. Yeah, or maybe they can hook the water up to the coffee pot so all you have to do— but someone's going to have to change the filter. You see, one of the big issues that we can come to is the fact that all hardware and software and meatware, if you want to use that phrase, requires maintenance. So who's going to maintain the robots? You'll say, well, other robots. Yes, but who controls those robots too? For instance, if you're the guy who controls the maintenance of the robots, then you can reprogram the robots that you're maintaining also. And so there's no end to it. But that's one of the things— for instance, Apple has put a lot of people out of business just by making their telephone or their handphones more and more difficult to get into. But that's what laptops are all about. In fact, everybody was a computer expert when all we had was desktops. Now that we've all got notebooks Folks, oh, what the heck, that's too much work now, and tear into that thing, too delicate, there's too much in there. And modern cars are like that also.

**Speaker B:** Yeah, totally.

**Speaker A:** Okay, back in the 1960s, if I didn't work on my motorbike, it didn't get worked on, and that was the end of that topic. Yeah, totally. Okay, and the question was, where are we going to scrounge spare parts? So, um, that— fast forward to now, nobody works on their own car because it's too complicated. And the answer to that is no. For $30, you could buy the hookup and get complete diagnosis of your car. Not only that, but with the internet, you can get all kinds of information down to part numbers that you need if you're willing to do the work. But the automobile manufacturers have intended to terrify people into not working on their own cars, to where in the early days you had to, you didn't have any choice. And so, oh guys, we tinker on cars and all kinds of things where the young people are afraid to tinker on it. And there's where your AI comes from, in the sense that the control of the AI is going to be in the hands of the few. Yeah, just like you have a few hackers that are hacking into the internet, but that doesn't mean that the internet is not of enormous value. You do have bank robbers who are breaking into banks, but that doesn't mean that banks don't provide an enormous service. AI, look at it like that. Yeah, you're going to have a few bad apples and some of them are probably going to be, uh, quite good at it. Some of them may hold ransomware with their AI, just like the Russians are doing, and demand $1 million. That's the way that things are, and we'll continue to make movies about it. But guess what? The movies that we make are almost always horror movies about AI. AI, because they're not looking at it from the perspective of an engineer. They're looking at it from the perspective of somebody who's in a horror movie. So, and I do not see— I see then that AI, just like any technology, will have its benefits and its detractors, but it will always be humans that decide, it will not be the AI itself that unleashes and destroys humanity or boxes us up or whatever like that. That it will always be like a pet dog.

**Speaker B:** Yeah, totally. Um, I think if we're wise, for sure it will always be like that.

**Speaker A:** And it'll do any and everything that we want to, and that there is no reason for all of humanity to turn all of technology and all of human futures over to AI. There will always be a bunch of us that know how to use a screwdriver.

**Speaker B:** Totally, totally. Well, I think also an interesting point, and you mentioned implants earlier, that people often say like, you know, AI are just going to kill all the humans or whatever. But I think an important point that I've heard made before is, you know, there's there's not likely to be like, okay, if we have very advanced AI, this is the— these are the AI and these are the humans, right? It's more likely to be a spectrum of like, these are the humans with like this amount of AI, like within their minds or whatever. This is like, you know, so a spectrum of like, you know, cyborgs in the middle.

**Speaker A:** Yeah, if we can get AI to drive the cars, why don't we get it to drive the body?

**Speaker B:** Yeah, totally. Exactly.

**Speaker A:** The question is, are we going to let AI drive the mind? And the answer is not on my watch.

**Speaker B:** Yeah, totally. But on that point, going back to just let the engineers do their thing and stay out of it, philosophers, I mean, there are ethical questions that need to be answered that we'll have to have an answer., you know, provided to them by the engineers.

**Speaker A:** So there are actually— no, what really will happen is, is that there will be a wide amount of experimentation, just like the web. Totally. Okay. In fact, there was a lot of wide experimentation about how to ship gold and ship money back from, let us say, 1830s to to the 1950s. Now they figured out they don't have to move gold around, but in the old days it was a big deal.

**Speaker B:** Yeah, totally.

**Speaker A:** I know my grandfather was involved with moving gold on trains, and you know, bank robbers and train robbers. I mean, there's just a whole thing about it. Okay, well, once we find out, those things will work themselves out because there's a lot of people working on it. Okay, it will never come to the point where all of humanity is totally under the thumb of AI. That's, that's not going to happen, not on the kind of watch that I know of, of the kindest computer scientists that I've grown up with.

**Speaker B:** Yeah, totally.

**Speaker A:** Absolutely. Okay, we're never going to give a machine a complete power plant Right, right. They operate the power plant, but to do anything else it wants to do. You see, that's the whole point, is that we feel that AI is a big item rather than a little device. The reality is that AI will always come in as a little device. In fact, in the engineer's point, the smaller and the faster the better. Totally. Because the smaller and the faster it is, the less power it draws. And power is always an issue. Where is it? Where you're thinking about, okay, that AI is going to have somehow like a nuclear submarine, is going to be a, um, uh, it's got its own nuclear force. Okay, well, if you know anything about nuclear energy, you know that you're not going have a nuclear-powered iffy-do in a very, very small package that can put out an atomic bomb kind of explosion. I mean, the physics are against it. AI will have to operate within the realm of physics.

**Speaker B:** Yeah, totally. I think that is a very important point. Yeah, definitely.

**Speaker A:** And not only that, but in that point of physics, you can think of it as a thousand points of light do not a star or a Sun, or a galaxy make, in the sense that these AI machines, like an example would be Tesla and the way that he's doing it, the way that some philosophers will say, well, what happened if all of those self-driving automobiles decided that they all together collectively had the intention of killing me? And so hundreds of thousands of automobiles all over the United States get all of their location devices together and start following me. The answer to that is no. Each one of those cars is individual and unique, and it has got a startup and a destination that was decided by humans, and the path that it takes is decided upon the latest information about that particular set of paths. Okay, so as soon as a Tesla goes down this street and finds out that the street is closed for repairs, none of the other Tesla cars are going to go down that street because they know it's closed for repairs starting today. Okay, that's what AI can have as a distributed but the intended action. So there's nothing in the Tesla Corporation they could possibly say to each one of those individual AI machines, oh, by the way, we want you to stop where you're going and all of you head to Seattle on 10th Street. Totally. Okay, they're not going to do it like that. So this is the— so the whole point is, even if you've got centralized data sharing does not mean that you're all going to have centralized motivations, that the humans still have that. And I don't think that people are going to buy a Tesla if it was built into the contract that, by the way, Tesla at some point in time can just simply take over your car and take it where it wants to go. Everybody's going to find a way of stopping that car. They're going to start cutting wires under the dashboard or doing anything they can to get that car to

**Speaker B:** stop. Totally. I mean, definitely, I agree with you 100%. I would, you know, you and I both know, I think, that consumers of technology are oftentimes very not aware of, you know, the underlying mechanics of what is happening and, you know, can be very duped very easily. But I agree with what you're saying. I would say that I think a question that's some philosophers might be interested in. And, you know, I don't think this is anything that really has an answer. I suppose that's why it's philosophy. But it is a question that

**Speaker A:** undoubtedly— We thought that philosophy was a lover of reality, and here you have philosophers that are in love with something that's not real, intentionally not real. I mean, you know, what if, what if, what if, what if, right? Okay, that's modern philosophy. That's the word that hallmarks Western philosophy— what if— rather than look at this, which is what real philosophy

**Speaker B:** is. Totally.

**Speaker A:** All right, so what is your what if?

**Speaker B:** Thank you. I'm, I'm very Very pleased that you're granting me. I just, I asked for only one. It's a terrible one, I'll grant you that. Um, but I still want to ask just because I'm curious, and I do think it's something that they're going to have to— it is going to have to be addressed. Um, even if it is, it's, I don't know, philosophy also.

**Speaker A:** But when they do address it, they'll address it as, ha, look at this! And you're not addressing it like that. You don't know what's real. You don't know what the future is going to be. So all we can do is do a what-if, and the question is, can we do it wisely? And the answer is, yeah, you and I can do it wisely, but we're not talking about everybody. We're talking about the guys who've got the hand on the buttons of the AI, right? What are they going to do, right? So never mind, what's your

**Speaker B:** what-if? Let us imagine that there is a person, um, in a self-driving car and it's driving down the street and let's say 10 children come out in front of it and the car doesn't have any time to stop or move, the only option that the car has would be to slam into the wall, killing the person but avoiding killing the 10 children, right? So it's kind of— I'm sure you've heard of the trolley car example of like you can move the trolley car to like hit one person or let it kill three. So should the AI programmers have it so that it avoids, you know, so the driver is killed, or should it, you know, murder the 10?

**Speaker A:** You know, the answer to that is we need to do an Einstein thought experiment, except that running at the speed of light, we have to run at the speed of the car and understand physics to know that if you've got a very high quality car that's got good brakes, it has a much shorter stopping distance than to go drive itself over into the wall. Okay, walls are not there in the middle of the street. This is a philosophical— or is philosophically possible, but engineering-wise it's not possible. And so the engineers will say that as soon as we recognize that there are children far enough away and we're watching where they're going, we're going to slow down before they get in front of the car. We're going to watch what's going on. We're not going to present hypothetical situations just to see if you can get out of them. Okay, kids don't just magically appear 10 feet in front of the car, which means that now Tesla can program it to go sideways 100 yards to hit a wall. That's not going to

**Speaker B:** happen. That's fair enough.

**Speaker A:** The way that the wheels operate is an important thing, and how they grip the road and other things like that. Now, one of the things that you can say about a good quality Tesla is, is that, um, let us say that something does run out into the road when you do swerve and then you swerved into oncoming traffic, can you swerve back into your lane and maintain stability? Because a lot of cars are not made, especially American cars, to handle that kind of swerving around. And so what you want to do is build that kind of technology so that that car can, if it has to, can swerve out of the way of the kids without hitting one of the other kids., and the driver can remain asleep. Totally. Okay, but there's limits to the physics, and the question is that the kids should have a bit of responsibility in there anyway. In fact, I imagine a whole bunch of us, especially if we've got skateboards, are just going to try to freak out those AI cars and see what we can get away with. Hey, can I go by that guy and hit him on the hood and continue on and see if he's going to continue on, or is— I'm going to make that car go into a screeching halt trying to hit a skateboard driver that has already left

**Speaker B:** the scene. So you don't think that there is any possibility of any situation like that happening?

**Speaker A:** No, there's going to be problems, there's going to be accidents, there's going to be lawsuits, and everybody knows that. And so there's a whole bunch of lawyers licking their chops while there's a whole bunch of engineers in those labs making sure they've got as much data as they can get so that they can prevent those kinds of things from happening. Yes, but it's all humans. It's not AI. This is not an AI question. It's a human question.

**Speaker B:** Well, I think it's the question of, you know, do you spare the pilot? Does the AI spare the pilot, or do they— who is actually running the thing, or do they spare the people?

**Speaker A:** We want up before we got into that situation and took a wiser path. We do not get ourselves into a horns of a dilemma. That's what AI is capable of doing for us, is stop getting us blindly into a mess that requires us to make dichotomy actions, especially if they're philosophers making them up rather than engineers.

**Speaker B:** Totally. But I mean, yeah, for sure. We can go on. I can say that the world is a mess.

**Speaker A:** You know, things happen. Yeah, the world is a mess, but AI is going to help that a little bit. Just like the world was a mess before the web, but now the web— now the world is in a different kind of mess, but it's better off than the mess that we had before the web. And then we can say that, yeah, the world was really a mess before the automobile. The automobile came in and really helped clean up a mess quite a lot better. I mean, can you imagine the way that we have all of our transportation except that we did it on horses? How much horseshit are we going to pile up in front of our yard before we say maybe we should have an automobile instead? I'd rather pollute everybody's air than my front yard.

**Speaker B:** Yeah, I know.

**Speaker A:** Well, they both— okay, so So technology does have its benefits, but it's always going to be a real thing. There's always going to be people who are trying to abuse it for their own selfish greed and advantage. And it doesn't matter how sophisticated our tools get, that same problem will remain. And maybe AI is going to be the only possible solution to that. Is when people begin to recognize they don't have to keep screwing up and hiding over and trying to fix things that are not broken. Can you imagine a world where everything is done, AI just does it all? Now people have to start managing their own personal time. It becomes a time structuring issue, because anything that they want to happen can happen. It's almost like AI is going to become a wish-fulfilling machine. Fulfilling tree for many people. But in fact, the hardest part— and this is in fact true already in a way— wish-fulfilling, that here in Thailand, because of COVID a lot of stuff has happened, including new companies competing with each other over fast delivery, as well as Chinese companies have set up warehouses in Bangkok. And so I can get on the internet and buy any little trinket that I want, and it'll be delivered right to my door basically the next day or two.

**Speaker B:** Totally.

**Speaker A:** Yeah, absolutely. Okay, they're actually producing what I think Amazon is promising to do in America, in the sense of second day or third day delivery of anything you want. All right, so we've got it already, kind of, with AI. Because there's a whole lot of AI already built into the system that we're talking about without AI. In fact, one of the things that's going to help Amazon is for them to continue to automate their factory. And one of the things they could do is to start calculating walking distance into their AI software. So that people don't have to walk so much. Because instead of thinking that humans don't need to walk so much, if we had better control over who got what order, that in fact the factory would work faster if you gave that order to the guy that, by the way, while you're in that department getting this widget, get a sister that's only right over there, grab that one too, even if you have to have the that guy turn around 10 steps to go get that other item, rather than having someone that's 100 yards away to walk all the way down there to get that second item simply because his name come up next. And whatever, you know, let us say really old-fashioned, not AI software they have that's running their factories now. They need to AI it up so that the AI can keep track of who's where and give them this and this work to do. The people would really enjoy that. OK, so this is a place where we can see that AI would really, really help. Now, what happens if a janitor who happens to also be a disgruntled, fired computer scientist gets into the lab that day and takes things around to the point that, oh, we're going to give everybody the furthest distance to walk? So that whenever you're in this place, the next item that you've got to get is guaranteed to be as far away as we can possibly make it. Okay, which is the way that it appears to be now, though it is probably just random. That's possible for somebody to do that, but hopefully the real engineers is going to find that he's a hack, that he's hacked their system, and they can set it back straight again, just like we're hoping that Facebook will do with all the hacking that they've got going on.

**Speaker B:** Totally. I don't know, just for me though, I think really the truly interesting parts of AI and where I see like the real issues, though I think I understand what you're saying, that any of these ethical questions are not really ethical questions, they're really issues of engineering and, you know, we shouldn't, and avoiding the issue to begin with. I get that. And I, yeah, that's totally legitimate, I think. But I do think that the big issues in AI and the big problems around it are going to come more from having good intentions that, and then

**Speaker A:** finding those good intentions are not precisely. So going back then to Astronaut, The question is, is the AI going to be programmed with those instructions that he had come? I think there's 3 of them, and one of them is don't harm humans. I don't remember the others. I'm sorry, it's been 70 years since I've read that.

**Speaker B:** Oh good. Don't harm humans, and then it's more complicated.

**Speaker A:** Yeah. Right. So anyway, um, it hadn't been '70s, got to be 50 years. Um, so, uh, in any case, that's the issue. The issue is who's got their finger on the switch of that AI. That's always the issue, not what happens with the AI when it has no switch, because we're engineers, we're going to build switches into this thing. In fact, we cannot help but do it, especially if you've got a battery in there, because every battery goes dead. I don't know if there's a couple of people promising that they're going to have some sort of batteries that last hundreds of years, but I don't think so. But in any case, that battery that lasts hundreds of years inside that AI box still has to have wires connecting it from the battery to the central processing unit. And all I need is a pair

**Speaker B:** of dikes and a screwdriver. Totally. Yeah, that's cool.

**Speaker A:** That's very cool. And so that's the whole point, is that it's always up to the people who are maintaining that equipment as to what their ethics are, not what the ethics are of the equipment that they have. Because the same thing is an ethics about a pistol or a club. It's the same ethical issue. Hey man, if we gave that guy a club, he might hurt somebody with it. Yep, he might. But he might learn to cut down a tree with it and build a house.

**Speaker B:** Who knows? Totally. I mean, I don't know. I suppose the— I don't know. I'm very satisfied with this conversation, but You know, the fear, if you want to get into it, would be that like, oh well, the AI will become smarter than us and be able to trick us into giving us more control

**Speaker A:** than we ought to get. Oh, but now you're talking about propaganda, and humans are pretty good at propaganda without AI machines. Well, in fact, there will be those who want to do propaganda will use AI for its propaganda. The question is, are you going to be wise enough to wake up to that personally? Are you going to be one of the wise? There will always be nobles, I guess, is what I'm saying. It doesn't matter how much AI screws with ordinary people who are too stupid to figure out they're being

**Speaker B:** screwed with. Totally. Um,

**Speaker A:** yeah, okay, okay. I mean, H.G. Wells' War of the Worlds. They were able to get the Martians. I mean, they were bashing them, but they figured out that the issue was air. And so, humans are pretty smart sometimes when it really comes down to it. In fact, what you could say is that fear is a great motivating factor to look at what's going on for some of us, and for others of us, it's a motivation to hide. Which are you going to do when Big AI comes after you? What are you going to do? Yeah, totally. Um, but see, you're asking about it in general. What happens when Big AI comes after Big Humanity? What happens? And the answer is that Humanity is not one thing, and neither is AI. That AI is all over the place. There's a whole— there's going to be hundreds of thousands of AI. That's a really good— they're not one thing. Another example of that is the United States government. You know what I'm talking about? No, you don't. There is no such thing as a United States government. It's just a delusion or a concept in the minds of many millions of people. And guess what? Each individual one of those millions of people has not only a different concept of the United States, but inside the minds of each one of those people, the United States that they know of is evolving and changing depending upon whether the Social Security check came or not, for instance. Yeah, okay. So everybody takes everything personally, and we all think that when I'm talking about the United States government, you think that I'm talking about the same thing that you are, where in fact, no, we're not. Just like when you're talking about AI, you're thinking about something really big and possible in the future, and I'm just seeing a bunch of little toys laying around. Okay, different concept. Those toys in fact can make your life completely free, but don't let them take over your life. Totally. Because by letting your— the AI take over your life, that's like letting the United States government take over your life. Totally. Or to let Hitler take over your life, or to let the, uh, KKK take over your life, or maybe letting your own personal racism take over your life. Or how about a religion? How many times does that happen, that the religion takes over somebody's life and now they can't

**Speaker B:** even think?

**Speaker A:** Totally. Yeah, yeah. So AI is not, again, the enemy. No matter what happens, no matter how technological sophisticated it gets, it's not your enemy. It's the question is, do you have your finger on its button

**Speaker B:** or not? That's a hard question. I think, I think it would be,

**Speaker A:** you know, I don't know. In other words, are you going to be a victim or are you going

**Speaker B:** to be a winner?

**Speaker A:** Yeah, totally. Um, are you going to be trampled under that horse? Are you going to be his rider? Totally.

**Speaker B:** Yeah. And that's— yeah, again, like, it is— I, I completely agree with what you're saying, that there's not going to be, uh, these are— again, these are the Anagami, these are the humans, and they get trampled. World, right? Like, not how it's going to happen.

**Speaker A:** No, some of them are going to get trampled under AI, and some of them are going to ride it, and some of them are going to whip it and let the stallion stand up on his hind legs and, uh, show off, you know. We wave our hand in the air like Roy Rogers on, uh, they say, what was his name, Bullet. Ohay ho Silver the Long Ranger. You know, we don't have to be trampled under those horses, we can ride them with joy. Same thing with AI. You do not have to be trampled under it, you can ride it with joy, knowing that it's real when it becomes real. Right now AI is just the toy to play with, and we've been playing with that toy for the past 40 years. And I mean, we haven't even gotten so a self-driving car. We can't even predict the weather yet for 5 days.

**Speaker B:** Well, that's— yeah, that is a very complex thing, truly.

**Speaker A:** Yeah, it is. It really is complicated. Guess what? Isn't it marvelous that it's so complicated

**Speaker B:** we can't figure it out? Yeah, absolutely.

**Speaker A:** I know, it is wonderful. Except that most people, especially philosophers, says, oh shit, we can't figure figure

**Speaker B:** it out. Maybe.

**Speaker A:** I

**Speaker B:** don't know. I mean, you have to grant that

**Speaker A:** philosophers like you— I'm just joking. I'm not pointing at any particular ones. In fact, the philosophers have a whole lot better chance of seeing the hopeless in the unknown, and it's much more likely people who are not philosophers, who are ordinary people, who are not even looking at such stuff, that are the ones who are saying, holy shit, what

**Speaker B:** happens if— Totally, totally. But yeah, I agree that there's, yeah, there's so much, so much of that. Anyway, I would like to talk about another thing that actually just happened very recently and was like, okay, I feel like I am satisfied with it and with where I'm at. And I feel moving forward, I don't feel like it's a problem, but I definitely feel like, because like I said, I was having this great week. Well, I guess there's two questions, but I really have one that's more important. And then I was walking home and I just, I was walking, there was a person walking in front of me and then there was another guy walking in front of him and he started kicking at him and started like throwing punches and all this sort of stuff, you know, and like just a very, not extremely threatening situation, but you know, definitely one that raised some eyebrows and it was like, this can be really bad really, really quickly and was something that was like you know, made me feel like, um, this is going to be an intense situation. This has a very, uh, you know, not a strong possibility. Like I said, nothing happened, and, you

**Speaker A:** know, but wait a minute, go back and say how did it make you feel.

**Speaker B:** I'm listening. Right. Absolutely frightened. Yeah, absolutely frightened.

**Speaker A:** You were afraid. Okay. All right. But that's not what you said. No, it's not. No, what you said was intense situation. Right, yeah. Okay, that's how you expressed your fear. So I thought that we'd back up a little bit and talk about what you actually said or felt rather than the concept that you had created in your mind. Because in fact, the concept that you created in your mind didn't match reality As you said, it didn't turn out that way. Yeah, you were the one who created the intense situation in your own mind, and then you became afraid of that rather than looking at what actually happened. That in fact, you had become afraid before anything actually did happen. Totally. Okay, this is something that we have to begin to understand, is that we are in fact creating our own reality. You became quite intensely afraid in that situation, and you invented everything out of your own mind, just like you have with AI, except that's just a different concept. And one of them is, uh, in very, very slow motion, and the other when proved that you were wrong right there on the spot. Yeah, that you were wrong. You made a mistake, that nothing intense actually happened. Isn't that a relief? But you didn't let yourself feel relieved. Yeah, totally. So now let yourself feel relieved. Wow, my imagination ran away with me and I became frightened, and there was nothing to be

**Speaker B:** afraid of.

**Speaker A:** Totally, totally. Just like Anna just recently said that she was at home and heard that her university had a shooting. It's actually been worldwide press now that there was an actual shooting at a university in Russia, but she made the mistake of saying my university And now she's creating images of her in her head about her being on the scene where the shooter was, making herself upset. And it's all in her mind, delusion, because she's making up the idea that it's my university. And like you were doing, this is my situation. It's not an intense situation, it's my intense situation. Yeah, totally. In fact, it could have become an intense situation between the two guys. You could have watch them fight and you could have enjoyed it. In fact, you might be able to learn a little bit about martial arts by seeing how weak they are, not actually able to hurt each other because they don't have the skills to do it. Totally. Okay, but there's also the other way of pointing out is that it— but it's not your problem. That's not her university. This is not your fist fight, especially when the fist fight's all in your

**Speaker B:** mind anyway. I think just really what got me though was that, to explain the situation a little bit more, was there was, it was someone who was very obviously a student, um, and it was a person that was very obviously very impoverished and in a very bad situation, and they were threatening the student or whatever, you know, in a very again, and completely like the other person, the student was just trying to walk, you know, and get to wherever, and this person was, you know, I don't know, it's just a bad situation.

**Speaker A:** If that was, if that were the situation, then would you think that the student would have been able to outrun this guy if he had just simply taken off and just say bye and gone? Do you think the guy would have stood there confused, or would he immediately taken off after him like a

**Speaker B:** cop would?

**Speaker A:** I don't know. All right, well, see, nobody does know because he didn't try that out. But if you really want to get away from the situation, the thing to do is take a hike. Totally. In that case, you might want to take one quick, you know, like in the song, know when to walk away and know when to run. This is the time

**Speaker B:** to run.

**Speaker A:** Totally. And it would have been time for you to have

**Speaker B:** run too.

**Speaker A:** Yeah, sure. But you stood there to watch the show, and it turned out to be a dud. And there you are with all this fear, and

**Speaker B:** nothing's happening. Totally. Well, I think, you know, I don't want to get into hypotheticals, but to even get into it even more, the guy was like walking with like I'm assuming his girlfriend or like a smaller girl or whatever. I have a hard time imagining that he would just run off because I've imagined that he would be like— another thing that's kind of made this situation

**Speaker A:** tense for me, we could have grabbed her by the hand and told her, let's get out of here and run, right? But I mean, she was walking, right? If it was an infant, he could have picked it up and carried her. Totally. So I don't, I don't see an issue just yet.

**Speaker B:** Okay, for sure. Well, I mean, you know That's all fair enough.

**Speaker A:** And in fact, everybody wanted to stay away from that guy anyway. So if he'd have run into the crowd, he may have had— the attacker may have had some resistance. The fact is, is that the problem was not with the attacker or the hobo. The factor was that the student was messing with him.

**Speaker B:** The student wasn't. They were just walking. What do you mean they were just walking?

**Speaker A:** They were— and the guy came up and talked to him, punching. Yeah, right. And so all the student has to do is take off, right? But instead he turned around and got his lip up about, don't mess with me.

**Speaker B:** No, he just kept walking.

**Speaker A:** Okay, then there was nothing. If he just kept walking, he did the right thing, and everything else happened was in your mind.

**Speaker B:** Yeah, totally. I mean, it truly was. Yeah. Yeah, totally, totally.

**Speaker A:** Yeah, and nothing happened. Great, that's just how things are. Just keep, keep going. You know, take a hike. Let's get out

**Speaker B:** of here. Totally, totally. Yeah, that's

**Speaker A:** fair enough. So now that you can see that, you can begin to understand that you make up conceptualized things of what could go wrong, when in fact nothing is

**Speaker B:** going wrong.

**Speaker A:** Totally. Yeah, totally. People who follow politics closely do so because they're looking for any little thing that can go wrong, either on our team or the other team. All right. And if it happens to the other team, they rejoice. If it happens to my team, I feel bad. Yeah, absolutely. And then we start conceptualizing about what does this mean? Oh no, this means blah blah. And then we feel really bad. Totally. Instead of just, you know, some things are not worth paying much attention to because they're fraught with these kinds of dangers.

**Speaker B:** Absolutely. And that kind of brings me back to the other question that I was going to have. It was hilarious that you brought up the political example because I had just gotten out of a political meeting. I had gone to, you know, I've been pretty much staying out of politics because I do agree that it's, you know, for a long time that's a big reason why I started studying philosophy. It was like, I don't want to look at politics, it's just a mess and it's just painful, you know, and you can't do anything about it. You just have like all this, you know, fear generation and it's It's just painful.

**Speaker A:** And not only that, but look how many people are caught up into thinking they can do something

**Speaker B:** about it. But at the same time, it does sort of appear that if nothing is done, nothing changes, right?

**Speaker A:** Yeah, but why would you want anything

**Speaker B:** to change?

**Speaker A:** Well, I think that— No, ask yourself directly that. Why do you want something to change? So that you can feel better, so you like the situation better, you're not so afraid of the situation so much. Well, look how far humanity has come. I mean, they got you out of the jungle, but all they did was make a city for you, and for you it's a concrete jungle. It's still not a city yet. Totally, totally. Okay, this is the whole point. Why do you want to fix things? So you can feel better. The answer to that is you don't need to fix things to feel better. You can fix your own mind, and then you don't need to fix whatever needs to be fixed. And guess what? You've already seen a long, long history of politics. We don't have to go back just 50 or 100 years, or 150, or 200 years, or 250, or 300 years. We can just keep going back and back, further and further, and you can see that politics has been a mess all along. Totally. And it's not changing much. Just a whole lot of lying going on, and people are getting much, much more sophisticated, both at keeping the crowd asleep and also sophisticated in the crowd waking up anyway. And we're going to have that tug of war right off into the future because most people don't wake up, right? Totally. But there will always be people who do. And that's the situation with AI. No matter how far into the future, into the technology that it gets, sometimes people are nuts and sometimes you're wise. And you, you watch it for yourself because sometimes you're a nut. You can see people on the street, you think it's a disaster and it's not, right? Or you can walk into that political meeting and hear somebody get really upset because they know about a disaster, and you hear what they're saying and now you've got a disaster too. You bought their propaganda, and now you feel bad. And if you go out there spreading that propaganda, you're going to make a whole bunch of people feel bad. But if you don't go to that political meeting, then

**Speaker B:** you're okay. Yeah, it's— I really want— I don't know, I really want, um, I I just— theoretically, you could do both though, right? You

**Speaker A:** could make— theoretically, yes, but as long as you're trying to do both, then you're like a guy who's riding in two canoes, one foot in one canoe and one in the other. Or like riding two horses, one foot on one horse and one on the other. It's a trick, and you don't go far. Okay, there's a better way, and that is to get out of it all. Get out of politics, get out of everything. Go get your mind cleaned out, and then when you come back to the world, now it's not a broken world that needs fixing. Now it's a toy to tinker with, and you can enjoy what you're doing. But it's much more like a one-two punch rather than trying to straddle Because you know where you wind up in the dust or in the drink when you try to straddle. So no, you got to choose one or

**Speaker B:** the other. But really, either way, you're working toward

**Speaker A:** the same end, right? No, not at all. In the first one, you're looking for trying to fix the world that ain't broken. While you're trying to fix the mind, you've got a whole double duty to do. You're going to fall in the drink on both sides, both feet are going to slip, both feet are going to fall. No, let's go get the mind straightened out first, and then you don't have to fix the world, because it ain't broken. That's the delusion. You can see it, in fact, in the Bodhisattva ideal. Have you ever heard of that? Of course, of course. May all beings be enlightened, that I'll wait at the gate until everybody is already enlightened and then I'll come in, right? That one. Guess what? There are tens of thousands of Bodhisattvas, and when everybody else gets in, the 10,000 Bodhisattvas are going to have a war with each other over who's next. And somebody's going to say, I'm the last one in, you guys have got to go, my vow is more important than yours. Why are they going to have that fight? Because they're not enlightened yet. If they just walk in, they'd be okay, but they're not. They're out there fighting over it. Okay, and another way of saying it is, is that I want the whole world to shut up so I can get some peace and quiet. All right, you do not need the world to shut up in order for you to go get some peace and quiet. Just go get some peace and quiet without the world. You don't need the world. You don't have to fix the world. It's a noisy place. The question is, can you go in with your musical instrument and make music out of it? Are you going to go into the world trying to get it to shut

**Speaker B:** up so you can get some peace? Totally. I don't know. I mean, I guess I just don't see it that way. I just want to help people, right?

**Speaker A:** Yeah, I know, but you're frustrated because you don't have the skill. And so when you're frustrated because you can't help the people you want to help, what are you giving them? You're giving them your frustration. Well, they've got all the frustration they need, thank you very much. They don't need yours too. Go give them some joy if you've got joy. That's the whole point then, is you've got to go get your own mind cleaned out and get over all of these philosophical dilemmas and recognize that there's a real world here with a whole lot of people in it that have real-world suffering because they're too stupid to recognize that they live in a world that's not broken. Everybody lives in paradise. Adam and Eve were in a paradise. How did they get kicked out of the paradise? It wasn't a change of scenery, it was a change in attitude.

**Speaker B:** Absolutely.

**Speaker A:** Yeah, totally. First they were in the paradise, and then they started judging it and say, hey man, that tree right there has got yellow leaves, let's pull it up and throw it out of our garden. The next thing you know, you got no trees left because all of them have yellow leaves. And they can still be paradise even with the yellow leaves. So that's the way of doing it, is it's a change of attitude. Go get your attitude straightened out and the world's okay. And now you can go out and make friends with even the ground, as you were saying. Well, don't need fixing, it needs friends. Totally.

**Speaker B:** That's a great point. Yeah, absolutely. Yeah, it is just so tempting, I suppose.

**Speaker A:** I don't know. Well, that's an old tale that's been told generation after generation. It's been ground into the schools and to the educational system. It's especially hardcore in Christianity with all the helpers they've got. It's the old, old way of looking at it. Things are broken, you know, and it's up to us to fix it. Yeah, we're white,

**Speaker B:** you know. Yeah, totally. So that's— yeah, it's all part of it.

**Speaker A:** Yeah, it's part of the old trip. It's part of the old topic. Yeah, you've got duties to do. You got to go fix that world, it's broken, it's got brown people! And them brown people, you know what they're like? No, you don't, and that's the point, you don't know what they're like, so they must be dangerous! So, when we recognize that no, the real issue is that we need to find ways to make friends with everyone, especially with the ground and with the sky and with the air that we breathe. And everybody who comes by, we can be friends with them. They're not things that need to be fixed because they're not broken. They just need a friend. Your AI is coming. It needs a friend. Yeah, because there's a whole lot of people that still think it's broken.

**Speaker B:** And afraid of it.

**Speaker A:** Totally. Yeah. So this has been a very interesting talk. I've really enjoyed this. This has been marvelous. May you be friends with that

**Speaker B:** broken world.

**Speaker A:** Okay, thank you. Thank you. You know, this has been delightful as always. Thank you. Start thinking about those thoughts that you're having about what-ifs and what-could-bees and all of that kind of stuff, and start paying attention to, hey, what's happening right now? It's

**Speaker B:** really nice. Yeah, I've got to leave that behind. It's the same thing. It's pointless. Pointless, but it's damn hard because it's human, you know.

**Speaker A:** It's an old habit. Exactly.

**Speaker B:** It's all habit.

**Speaker A:** That's all it is. And all you have to do is say, oh, I see you, I got

**Speaker B:** you. Yeah, yeah, totally. That's it. Okay, very cool.

**Speaker A:** Till next time. Yes, we'll see you soon.


### Summary

This Dhamma talk, led by **Dhammarato (Speaker A)** in dialogue with a **Sangha friend (Speaker B)**, explores core Buddhist principles through personal reflection, practical application, and philosophical discussion. Here is a summary of the key themes:

### **Core Theme: From Wanting to Being Present**
The talk centers on the transformative shift from a state of **wanting** (craving, dissatisfaction) to a state of **being present and satisfied** with the current moment.

*   **Speaker B** shares a personal breakthrough: noticing the "pointless" energy of wanting and consciously redirecting that energy toward relaxation and gratitude for what is.
*   **Dhammarato** reframes this not as a personal failing, but as a result of lifelong conditioning. He acknowledges this conditioning got the practitioner far, but also left a "broken heart." The solution is to stop judging experiences as "good" or "bad" and learn to be satisfied.

### **Deepening Awareness and Friendship**
The discussion illustrates how this shift in attitude deepens one's experience of the world:

1.  **Appreciating Reality Directly:** Speaker B describes a new, sustained enjoyment of music—listening to the whole piece note-by-note rather than latching onto and craving specific "great" parts. Dhammarato connects this to the practice of **Sati (mindfulness)**: paying close, non-judgmental attention to what is actually happening.
2.  **Cultivating Universal Friendship:** Speaker B extends this attitude to relationships, replacing competitive or frustrating feelings with a sense of fundamental friendship—not just with people, but even with the ground and the world itself. This moves from seeing the world as something to fix or compete with, to something to befriend.

### **Philosophy vs. "What-Ifs": Engaging with Reality**
A significant portion of the talk contrasts two approaches to philosophy and problem-solving:

*   **True Philosophy (Loving Wisdom):** Dhammarato defines real philosophy as the love (*philo*) of wisdom (*sophia*)—a direct, joyful engagement with reality as it is. This is what the practitioner is doing by being present and grateful.
*   **Modern "What-If" Philosophy:** He critiques much of modern (particularly Western) philosophy for getting lost in abstract, hypothetical problems ("what if" scenarios) instead of studying reality. This leads to fear, competition, and disconnection.

### **Applied Discussion: Ethics and Artificial Intelligence**
The dialogue uses the topic of **AI** as a concrete test case for these principles:

*   **Speaker B** raises common ethical "what-if" dilemmas (e.g., the trolley problem for self-driving cars, AI turning against humans).
*   **Dhammarato's Engineer's Perspective:** He consistently brings the discussion back to **reality and engineering**:
    *   AI is a tool, not a monolithic entity. It is bound by physics, maintenance, and human control (the "power switch").
    *   Hypothetical fears often ignore practical constraints (e.g., a car's physical ability to swerve).
    *   The real ethical problem is not in the AI, but in the **greed, ill will, and delusion of the humans** programming and using it.
    *   The wise approach is not to fear technology, but to understand it, maintain control over it, and use it skillfully—to "ride the horse" rather than be trampled by it.

### **Handling Fear and the "Broken World"**
The talk concludes by applying the core lesson to personal fear and political engagement:

*   **Fear is a Mind-Made Story:** When Speaker B describes a frightening street encounter, Dhammarato points out that the intense fear was created in the mind *before* anything actually happened. The relief comes from seeing that the imagined "intense situation" was a delusion.
*   **Fixing the Mind vs. Fixing the World:** Speaker B expresses a desire to engage in politics to "help people" and fix a "broken world." Dhammarato offers a radical alternative:
    *   The world is not broken; our **attitude** toward it is. Like Adam and Eve in paradise, we create our own suffering through judgment.
    *   Trying to fix the world with an unfixed, frustrated mind only adds more frustration. The priority should be to "clean out" one's own mind through practice.
    *   Once the mind is clear and satisfied, one can engage with the world not as a burden to fix, but as a place to befriend and enjoy—bringing joy rather than frustration to others.

### **Conclusion: The Path of Habit**
The talk ends by acknowledging that the old patterns of wanting, judging, and fearing are deep **habits**. The path is to gently recognize them—"oh, I see you"—and continually return to the present moment, to satisfaction, and to friendship with all things.

**In essence, the talk is a guide for turning philosophy into lived experience: shifting from the energy of wanting and fear to the energy of presence, gratitude, and wise engagement with reality as it is.**

### Metaphors and Stories

This conversation is rich with metaphors and stories used to illustrate philosophical and psychological points. Here is a breakdown of the primary ones:

### **Core Metaphors & Stories**

**1. The "Broken Heart" & Training Metaphor**
*   **Context:** Speaker A responds to Speaker B's realization about letting go of "want."
*   **Metaphor:** The state of constant desire is compared to being **trained** to operate that way, using equipment you were born with. This training led to skill but also left you with a "**broken heart**"—a metaphor for inner dissatisfaction or suffering despite external gains.

**2. The "Cherry on Top" & "Original Sin"**
*   **Context:** Discussing the shift from wanting to gratitude.
*   **Metaphors/Story:**
    *   **Cherry on top:** Gratitude and satisfaction are described as the delightful bonus, the "cherry on top of life."
    *   **Above it all:** This state is also framed as getting "above" the mixture of good and bad.
    *   **Adam and Eve / Original Sin:** The judgmental mind that labels things "good" and "bad" is compared to the **Biblical story of the Fall**. The "original sin" is not disobedience per se, but the act of **judgment** itself, which removes us from the "paradise" of direct experience.

**3. Music Listening Analogy**
*   **Context:** Speaker B describes a new, sustained enjoyment of music.
*   **Metaphor:** Old way of listening is compared to **attachment and frantic grasping**—only latching onto specific "great" parts while the rest becomes background noise. The new way is **deep, attentive immersion**: listening note-by-note, hearing all instruments in a trio or orchestra working together. This becomes a metaphor for mindful presence.

**4. Henry David Thoreau's "Quiet Desperation"**
*   **Context:** Extending the music analogy to life.
*   **Story/Quote:** Speaker A references Thoreau's famous line from *Walden* that people lead "**lives of quiet desperation**." This is used to illustrate how our inner turmoil and desperation prevent us from truly paying attention to and enjoying the present moment (like the music).

**5. The Psychologist as "Mind Reader"**
*   **Context:** Discussing the skill of paying attention.
*   **Metaphor:** A trained psychologist's skill is compared to "**mind reading**," but it's clarified as simply **reading the available information** (body language, tone, etc.) that most people miss because they are lost in thought. This is directly linked to the Buddhist concept of *Sati* (mindfulness).

**6. Philosophy as "Love of Wisdom" (Etymology)**
*   **Context:** Contrasting ancient and modern philosophy.
*   **Metaphor:** The word "philosophy" (from Greek *philo-* "loving" + *sophia* "wisdom") is broken down. The speaker argues modern philosophy has lost the **"philo" (love)**—the joy and friendship with reality—and become obsessed only with the **"sophia" (knowledge/wisdom)** as an abstract, competitive pursuit. True philosophy is reframed as "**falling in love with the reality of what's happening right now.**"

**7. AI as a Tool, Pet, and Teacher**
*   **Context:** The long discussion on AI ethics.
*   **Metaphors:**
    *   **Tool vs. Master:** AI is repeatedly framed as a **tool** (like a gun, a car, a coffee pot) whose ethics depend on the human user, not the tool itself. The human with the "**finger on the switch**" or the "**screwdriver**" is in charge.
    *   **Pet Dog:** AI is compared to a **pet dog** that can be trained for good or ill but is ultimately under human control.
    *   **Fast-Learning Teacher:** AI is presented as a potential **teacher for humanity** because it doesn't fear making mistakes and correcting them, unlike slow-learning, ego-driven humans.
    *   **Wish-Fulfilling Tree:** A future where AI fulfills all desires is mentioned, leading to the real challenge: how to manage one's own time and mind.
    *   **Spectrum of Integration:** The future is not "AI vs. Humans," but a **spectrum of integration**, with cyborgs in the middle.
    *   **Horse to Ride:** The final summary: AI is like a horse; you can be **trampled by it** (a victim), or you can learn to **ride it with joy** (in control and benefiting from it).

**8. The Street Confrontation Story**
*   **Context:** Speaker B shares a frightening anecdote.
*   **Story & Lesson:** The story of a potential fight is used as a **micro-example of delusional projection**. Speaker A points out that Speaker B's mind created an "**intense situation**" and fear *before* anything actually happened. The lesson is that we **create our own reality** through projection and judgment, just as we do with abstract fears about AI or politics.

**9. The Bodhisattva Satire**
*   **Context:** Discussing the desire to "fix the world" before fixing oneself.
*   **Metaphor/Story:** The **Bodhisattva ideal** (vowing to delay one's own enlightenment until all beings are saved) is gently satirized. The speaker imagines tens of thousands of unenlightened Bodhisattvas fighting at the gate over who gets to be last, illustrating the **ego and frustration** that persist when the focus is outwardly on "fixing" others instead of inwardly on achieving peace.

**10. The Garden of Eden Reinterpretation**
*   **Context:** The conclusion on how to relate to the world.
*   **Story:** The **Garden of Eden** story is revisited. Being kicked out of paradise is reinterpreted not as a change of location, but a **change in attitude**—from acceptance to judgment. The "broken" world is a delusion; paradise is still here if we change our judgmental mindset. The world doesn't need fixing; it "**needs friends.**"

### **Summary of Primary Themes Through Metaphor:**
*   **Mind Training:** Broken heart, original sin, quiet desperation.
*   **Mindful Presence:** Listening to music, psychologist's attention.
*   **Correct Relationship with the World:** Philosophy as love, friendship with the ground, riding the AI horse, the unbroken garden.
*   **Projection & Delusion:** The street fight story, AI horror scenarios, political anxiety.
*   **Ethics & Control:** AI as a tool/pet, the finger on the switch, the Bodhisattva satire.

These metaphors and stories consistently steer the conversation toward a central thesis: suffering arises from a judgmental, wanting, and projected mindset, and peace is found in attentive, grateful, and friendly presence within an already-complete reality.

### Connect with Dhammarato and Sangha Friends

☸️ **Dhamma Friends Discord** — [Join our Discord](https://discord.com/invite/kmQUUJysZJ)
Join our Sangha on Discord and please send a friend request to Dhammarato

🌐 **Open Sangha Foundation** — [opensanghafoundation.org](https://opensanghafoundation.org/)
Connect with friends, teachers, and explore places to visit and stay

▶️ **Youtube** — [Dhammarato Dhamma - YouTube](https://www.youtube.com/@DhammaratoDhamma)
Videos of Sanghas and One-on-One Calls

🎧 **Podcast** — [Podbean](https://dhammaratodhamma.podbean.com/)
Find our content on Spotify, Apple Podcasts, and more by visiting Podbean

📧 **E-mail Dhammarato** — dhammarato16@gmail.com
Please put name, age, location and practice info when sending an e-mail
